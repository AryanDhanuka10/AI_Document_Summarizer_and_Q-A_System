LLM_PROVIDER=groq # using groq for testing by default

# LLM Configuration


GROQ_API_KEY=your_api_key_here
GROQ_MODEL=llama-3.1-8b-instant

LLM_PROVIDER=openai  # openai | groq

# Pinecone Configuration
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_ENV=your_pinecone_environment_here
# example: us-east-1-aws

# Backend
BACKEND_URL= # your_backend_url_here
